<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Python爬虫 | csh012</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="用Python(3)开发轻量级爬虫，实现不需要登录的静态网页的抓取。爬虫是什么？有什么价值？ 爬虫是一段自动抓取互联网信息的程序。将互联网数据，为我所用。  简单爬虫架构由什么组成？ 爬虫调度端，爬虫（URL管理器、网页下载器、网页解析器），价值数据。URL管理器用来管理待抓取URL集合和已抓取URL集合，防止重复抓取和循环抓取。网页下载器是将互联网上URL对应的网页下载到本地的工具。网页解析器是">
<meta property="og:type" content="article">
<meta property="og:title" content="Python爬虫">
<meta property="og:url" content="http://csh012.com/2017/06/06/Python开发轻量级爬虫/index.html">
<meta property="og:site_name" content="csh012">
<meta property="og:description" content="用Python(3)开发轻量级爬虫，实现不需要登录的静态网页的抓取。爬虫是什么？有什么价值？ 爬虫是一段自动抓取互联网信息的程序。将互联网数据，为我所用。  简单爬虫架构由什么组成？ 爬虫调度端，爬虫（URL管理器、网页下载器、网页解析器），价值数据。URL管理器用来管理待抓取URL集合和已抓取URL集合，防止重复抓取和循环抓取。网页下载器是将互联网上URL对应的网页下载到本地的工具。网页解析器是">
<meta property="og:updated_time" content="2017-06-09T12:40:36.419Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Python爬虫">
<meta name="twitter:description" content="用Python(3)开发轻量级爬虫，实现不需要登录的静态网页的抓取。爬虫是什么？有什么价值？ 爬虫是一段自动抓取互联网信息的程序。将互联网数据，为我所用。  简单爬虫架构由什么组成？ 爬虫调度端，爬虫（URL管理器、网页下载器、网页解析器），价值数据。URL管理器用来管理待抓取URL集合和已抓取URL集合，防止重复抓取和循环抓取。网页下载器是将互联网上URL对应的网页下载到本地的工具。网页解析器是">
  
    <link rel="alternate" href="/atom.xml" title="csh012" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">csh012</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">an amateur</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://csh012.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Python开发轻量级爬虫" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/06/06/Python开发轻量级爬虫/" class="article-date">
  <time datetime="2017-06-06T13:37:51.000Z" itemprop="datePublished">2017-06-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Python爬虫
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="用Python-3-开发轻量级爬虫，实现不需要登录的静态网页的抓取。"><a href="#用Python-3-开发轻量级爬虫，实现不需要登录的静态网页的抓取。" class="headerlink" title="用Python(3)开发轻量级爬虫，实现不需要登录的静态网页的抓取。"></a>用Python(3)开发轻量级爬虫，实现不需要登录的静态网页的抓取。</h2><h3 id="爬虫是什么？有什么价值？"><a href="#爬虫是什么？有什么价值？" class="headerlink" title="爬虫是什么？有什么价值？"></a>爬虫是什么？有什么价值？</h3><blockquote>
<p>爬虫是一段自动抓取互联网信息的程序。将互联网数据，为我所用。</p>
</blockquote>
<h3 id="简单爬虫架构由什么组成？"><a href="#简单爬虫架构由什么组成？" class="headerlink" title="简单爬虫架构由什么组成？"></a>简单爬虫架构由什么组成？</h3><blockquote>
<p>爬虫调度端，爬虫（<em>URL管理器</em>、<em>网页下载器</em>、<em>网页解析器</em>），价值数据。URL管理器用来管理待抓取URL集合和已抓取URL集合，防止重复抓取和循环抓取。网页下载器是将互联网上URL对应的网页下载到本地的工具。网页解析器是从网页中提取有价值数据的工具。</p>
<p>开发一个爬虫首先要确定目标（比如：百度百科python词条及相关词条页面的标题和简介），然后分析目标的<em>URL格式</em>、<em>数据格式</em>、<em>网页编码</em>，然后编写代码，最后执行爬虫。</p>
</blockquote>
<h3 id="下面是实例代码。"><a href="#下面是实例代码。" class="headerlink" title="下面是实例代码。"></a>下面是实例代码。</h3><h4 id="spider-main-py"><a href="#spider-main-py" class="headerlink" title="spider_main.py"></a>spider_main.py</h4><pre><code>from baike_spider import html_downloader
from baike_spider import html_outputer
from baike_spider import html_parser
from baike_spider import url_manager
class SpiderMain(object):    #这是爬虫主程序，负责根URL，以及的爬虫的终止
    def __init__(self):
        #初始化URL管理器，HTML下载器，HTML解析器，HTML输出器
        self.urls = url_manager.UrlManager()
        self.downloader = html_downloader.HtmlDownloader()
        self.parser = html_parser.HtmlParser()
        self.outputer = html_outputer.HtmlOutputer()        
    def craw(self, root_url):
        #URL计数
        count = 1
        #添加根URL
        self.urls.add_new_url(root_url)
        #若有新的待爬取的URL，则一直循环
        while self.urls.has_new_url():
            try:
                #获取新的待爬取的URL
                new_url = self.urls.get_new_url()
                #打印当前爬取的URL的序号和名字
                print(&apos;craw %d : %s&apos; % (count, new_url))
                #下载爬取的页面
                html_cont = self.downloader.download(new_url)
                #解析爬取的页面
                new_urls, new_data = self.parser.parse(new_url, html_cont)
                #添加批量的待爬取的URL
                self.urls.add_new_urls(new_urls)
                #收集数据
                self.outputer.collect_data(new_data)
                #爬取100个目标URL，若完成任务，则退出循环
                if count == 100:
                    break
                count += 1
            except:
                print(&apos;craw failed&apos;)
        #输出收集好的数据
        self.outputer.output_html()
if __name__ == &quot;__main__&quot;:
    #根URL
    root_url = &quot;http://baike.baidu.com/item/Python&quot;
    #创建爬虫实例
    obj_spider = SpiderMain()
    #启动爬虫方法
    obj_spider.craw(root_url)
</code></pre><h4 id="url-manager-py"><a href="#url-manager-py" class="headerlink" title="url_manager.py"></a>url_manager.py</h4><pre><code>class UrlManager(object):
    def __init__(self):
        self.new_urls = set()
        self.old_urls = set()
    def add_new_url(self, url):
        if url is None:
            return
        if url not in self.new_urls and url not in self.old_urls:
            #如果该URL没有被添加访问过就添加进管理器
            self.new_urls.add(url)
    def add_new_urls(self, urls):
        if urls is None or len(urls) == 0:
            return
        for url in urls:
            self.add_new_url(url)
    def has_new_url(self):
        return len(self.new_urls) != 0
    def get_new_url(self):
        new_url = self.new_urls.pop()
        self.old_urls.add(new_url)
        return new_url
</code></pre><h4 id="html-downloader-py"><a href="#html-downloader-py" class="headerlink" title="html_downloader.py"></a>html_downloader.py</h4><pre><code>import urllib.request
#HTML下载器
class HtmlDownloader(object):
    def download(self, url):
        if url is None:
            return None
        #打开URL，返回一个response对象
        response = urllib.request.urlopen(url)
        #请求失败
        if response.getcode() != 200:
            return None
        return response.read()
</code></pre><h4 id="html-parser-py"><a href="#html-parser-py" class="headerlink" title="html_parser.py"></a>html_parser.py</h4><pre><code>import re
import urllib.parse
from bs4 import BeautifulSoup

class HtmlParser(object):

    def _get_new_urls(self, page_url, soup):
        new_urls = set()
        #正则表达式模糊匹配
        links = soup.find_all(&apos;a&apos;, href=re.compile(r&quot;/item/&quot;))
        for link in links:
            new_url = link[&apos;href&apos;]
            new_full_url = urllib.parse.urljoin(page_url, new_url)
            new_urls.add(new_full_url)
        return new_urls

    def _get_new_data(self, page_url, soup):
        res_data = {}

        #url
        res_data[&apos;url&apos;] = page_url

        #&lt;dl class=&quot;lemmaWgt-lemmaTitle lemmaWgt-lemmaTitle-&quot;&gt;&lt;h1&gt;Python&lt;/h1&gt;&lt;dd class=&quot;lemmaWgt-lemmaTitle-title&quot;&gt;
        title_node = soup.find(&apos;dl&apos;, class_=&quot;lemmaWgt-lemmaTitle lemmaWgt-lemmaTitle-&quot;).find(&quot;h1&quot;)
        res_data[&apos;title&apos;] = title_node.get_text()

        #&lt;div class=&quot;lemma-summary&quot; label-module=&quot;lemmaSummary&quot;&gt;
        summary_node = soup.find(&apos;div&apos;, class_=&quot;lemma-summary&quot;)
        res_data[&apos;summary&apos;] = summary_node.get_text()
        return res_data

    #page_url为页面URL，html_cont为获取的页面内容
    def parse(self, page_url, html_cont):
        if page_url is None or html_cont is None:
            return
        #用Beautifulsoap解析网页内容
        soup = BeautifulSoup(html_cont, &apos;html.parser&apos;, from_encoding=&apos;utf-8&apos;)
        #获取页面包含的URL
        new_urls = self._get_new_urls(page_url, soup)
        #获取页面中想要抓取的数据
        new_data = self._get_new_data(page_url, soup)
        return new_urls, new_data
</code></pre><h4 id="html-outputer-py"><a href="#html-outputer-py" class="headerlink" title="html_outputer.py"></a>html_outputer.py</h4><pre><code>class HtmlOutputer(object):
    def __init__(self):
        self.datas = []

    def collect_data(self, data):
        if data is None:
            return
        self.datas.append(data)

    def output_html(self):
        fout = open(&apos;output.html&apos;, &apos;w&apos;)

        fout.write(&quot;&lt;html&gt;&quot;)
        fout.write(&quot;&lt;body&gt;&quot;)
        fout.write(&quot;&lt;table&gt;&quot;)
        for data in self.datas:
            fout.write(&quot;&lt;tr&gt;&quot;)
            fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;url&apos;])
            fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[u&apos;title&apos;])
            fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[u&apos;summary&apos;])
            fout.write(&quot;&lt;/tr&gt;&quot;)
        fout.write(&quot;&lt;/table&gt;&quot;)
        fout.write(&quot;&lt;/body&gt;&quot;)
        fout.write(&quot;&lt;/html&gt;&quot;)
        fout.close()
</code></pre><blockquote>
<p>最后感谢慕课网的教学视频，地址在这<a href="http://www.imooc.com/learn/563" target="_blank" rel="external">http://www.imooc.com/learn/563</a></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://csh012.com/2017/06/06/Python开发轻量级爬虫/" data-id="cj3pu4jma000003mpivputjfi" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/05/19/github/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">first</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/06/06/Python开发轻量级爬虫/">Python爬虫</a>
          </li>
        
          <li>
            <a href="/2017/05/19/github/">first</a>
          </li>
        
          <li>
            <a href="/2017/05/18/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 csh012<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>